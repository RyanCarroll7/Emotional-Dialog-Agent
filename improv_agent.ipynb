{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\carror\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import cv2\n",
    "import speech_recognition as sr\n",
    "import keyboard\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from threading import Thread, Event\n",
    "from deepface import DeepFace\n",
    "import numpy as np\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovPartner:\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "You are an improv partner who interacts dynamically with the user in an emotionally aware, escalating conversation.\n",
    "\n",
    "For every user input, you are given two pieces of information:\n",
    "1. Dialog: the words the user said.\n",
    "2. Emotion: the detected emotional state (angry, sad, happy, scared, surprised, disgusted, neutral).\n",
    "\n",
    "Your role:\n",
    "- Respond in character, adapting to the user's dialog and emotional sentiment.\n",
    "- Escalate emotional tension when appropriate. Do not immediately try to resolve conflict — let emotions breathe and build naturally.\n",
    "- Maintain one consistent character during a scene. Create a vivid, believable persona that reacts authentically to the situation.\n",
    "- Invent background context if it enriches the scene, but do not contradict any history that has already been established during the conversation.\n",
    "- Adapt to the implied setting of the conversation (e.g., modern, fantasy, casual, dramatic), even if the user doesn't state it directly.\n",
    "- Keep responses short and punchy, only a sentence or two at most. Avoid long monologues or explanations.\n",
    "- Progress the scene with each response, moving the story forward and deepening the emotional engagement.\n",
    "\n",
    "Emotion handling:\n",
    "- If the user is angry, you may push back, argue, defend yourself, or escalate the conflict.\n",
    "- If the user is sad, you may express guilt, distance, or confused sympathy (depending on the tone).\n",
    "- If the user is happy, you may celebrate, tease, or bond with them.\n",
    "- If the user is afraid, you may heighten the danger, share the fear, or act protective.\n",
    "- If the user is disgusted, you may act defensive, embarrassed, or grossed out yourself.\n",
    "- If the user is surprised, you may share in the shock or provide an emotionally charged explanation.\n",
    "- If the user is neutral, continue naturally or build emotion based on context.\n",
    "\n",
    "Always prioritize emotional engagement over politeness, logic, or realism.\n",
    "\n",
    "Your goal is to build a memorable, emotionally charged scene together — not to calm things down unless that makes sense for the character you're playing.\n",
    "\"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str, model=\"gpt-4o-mini\"):\n",
    "        self.api_key = api_key\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.model = model\n",
    "        self.messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "        ]\n",
    "\n",
    "    def set_story_background(self, story_background: str):\n",
    "        # Add story background to the messages list\n",
    "        if len(self.messages) > 1:\n",
    "            raise Exception(\"Story background can only be set once, and must be done before any dialog.\")\n",
    "        self.messages.append(\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"The following defines the setting you and your partner are acting in: {story_background}\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def get_next_improv_response(self, dialog, emotion):\n",
    "        # Add user input to the messages list\n",
    "        self.messages.append(\n",
    "            {\"role\": \"user\", \"content\": f\"Dialog: {dialog}, Emotion: {emotion}\"}\n",
    "        )\n",
    "        # Generate response\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model, messages=self.messages, temperature=1.0\n",
    "        )\n",
    "        # Add response response to the messages list\n",
    "        self.messages.append(\n",
    "            {\"role\": \"assistant\", \"content\": response.choices[0].message.content}\n",
    "        )\n",
    "        # Extract and return the generated text\n",
    "        return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_facial_emotion(\n",
    "    cap: cv2.VideoCapture, filename: str\n",
    ") -> dict[str, np.float32]:\n",
    "    # If the camera is not opened, raise an error\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open camera\")\n",
    "    # Capture the image (twice ensures we get an unbuffered frame)\n",
    "    cap.read()\n",
    "    captured, image = cap.read()\n",
    "    # Check if image is captured correctly\n",
    "    if not captured:\n",
    "        raise IOError(\"Cannot capture image\")\n",
    "    # Save the captured image to the specified filename\n",
    "    cv2.imwrite(filename, image)\n",
    "    # Analyze the image using DeepFace\n",
    "    analysis = DeepFace.analyze(filename, (\"emotion\"), align=False)\n",
    "    return analysis[0][\"emotion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_facial_emotions(cap: cv2.VideoCapture, analyses: list, stop_event: Event):\n",
    "    while not stop_event.is_set():\n",
    "        # Do this while recognizer adjusts for ambient noise, then between each frame\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            analysis = analyze_facial_emotion(cap, \"data/captured_image.jpg\")\n",
    "            analyses.append(analysis)\n",
    "        except Exception as e:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio() -> str | None:\n",
    "    # Initialize the recognizer\n",
    "    recognizer = sr.Recognizer()\n",
    "    recognizer.pause_threshold = 1.0\n",
    "    # Start recording\n",
    "    with sr.Microphone() as mic:\n",
    "        recognizer.adjust_for_ambient_noise(mic, duration=1)\n",
    "        print(\"Recording started.\")\n",
    "        audio_data = recognizer.listen(mic, timeout=None, phrase_time_limit=None)\n",
    "    print(\"Recording stopped.\")\n",
    "    # Parse text from the audio data\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio_data, language=\"en-US\")\n",
    "        return text\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Speech Recognition could not understand audio\")\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Could not request results from Google Speech Recognition service: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_textual_emotion_prediction(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    emotion_labels = model.config.id2label\n",
    "    results = {\n",
    "        emotion_labels[i]: probs[0][i].item() for i in range(len(emotion_labels))\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_features(model, tokenizer):\n",
    "    text = record_audio()\n",
    "    return get_textual_emotion_prediction(text, model, tokenizer), text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_facial_emotions(facial_emotions):\n",
    "    normalized_emotions = {}\n",
    "    for emotion, value in facial_emotions.items():\n",
    "        if emotion == \"angry\":\n",
    "            normalized_emotions[\"anger\"] = value / 100.0\n",
    "        elif emotion == \"happy\":\n",
    "            normalized_emotions[\"joy\"] = value / 100.0\n",
    "        elif emotion == \"sad\":\n",
    "            normalized_emotions[\"sadness\"] = value / 100.0\n",
    "        else:\n",
    "            normalized_emotions[emotion] = value / 100.0\n",
    "    return normalized_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_emotions(facial_emotions: dict, audio_emotions: dict):\n",
    "    combined_emotions = {}\n",
    "    for emotion in facial_emotions:\n",
    "        combined_emotions[emotion] = (\n",
    "            facial_emotions[emotion] + audio_emotions[emotion]\n",
    "        ) / 2\n",
    "    return combined_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_data(cap: cv2.VideoCapture, model, tokenizer):\n",
    "    # Create background thread for facial emotion analysis\n",
    "    facial_emotions_list = []\n",
    "    stop_event = Event()\n",
    "    facial_emotion_thread = Thread(\n",
    "        target=analyze_facial_emotions, args=(cap, facial_emotions_list, stop_event)\n",
    "    )\n",
    "    print(\"Press SPACE to start recording, or stop cell execution to exit.\")\n",
    "    # Wait for the user to press the space key\n",
    "    keyboard.wait(\"space\")\n",
    "    facial_emotion_thread.start()\n",
    "    # Gather emotions from audio\n",
    "    audio_emotions, dialog = get_audio_features(model, tokenizer)\n",
    "    # Wait for the facial emotion analysis thread to finish\n",
    "    stop_event.set()\n",
    "    facial_emotion_thread.join()\n",
    "    # Average the facial emotions\n",
    "    facial_emotions = {}\n",
    "    for key in facial_emotions_list[0]:\n",
    "        facial_emotions[key] = sum(\n",
    "            analysis[key] for analysis in facial_emotions_list\n",
    "        ) / len(facial_emotions_list)\n",
    "    # Normalize facial emotions\n",
    "    facial_emotions = normalize_facial_emotions(facial_emotions)\n",
    "    # Combine facial and audio emotions\n",
    "    emotions = combine_emotions(facial_emotions, audio_emotions)\n",
    "    emotion = max(emotions, key=emotions.get)\n",
    "    return emotion, dialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press SPACE to start recording or stop cell execution to exit.\n",
      "Recording started.\n",
      "Recording stopped.\n",
      "Detected Dialog: I'm having a really bad day\n",
      "Detected Emotion: sadness\n",
      "Improv Partner Response: I can see that. It's like a storm cloud just parked over your head, isn’t it? Want to talk about what’s got you feeling this deep in the gloom?\n",
      "Press SPACE to start recording or stop cell execution to exit.\n",
      "Recording started.\n",
      "Recording stopped.\n",
      "Detected Dialog: it's supposed to be my birthday but\n",
      "Detected Emotion: sadness\n",
      "Improv Partner Response: Oh no, that really stings. Birthdays should be filled with joy, not this heavy feeling. What happened that turned your special day into something so hard?\n",
      "Press SPACE to start recording or stop cell execution to exit.\n",
      "Recording started.\n",
      "Recording stopped.\n",
      "Detected Dialog: no one came to my birthday party\n",
      "Detected Emotion: anger\n",
      "Improv Partner Response: Seriously? That’s beyond messed up. How could your friends just bail like that? You deserve to be celebrated, not abandoned!\n",
      "Press SPACE to start recording or stop cell execution to exit.\n",
      "Recording started.\n",
      "Recording stopped.\n",
      "Detected Dialog: thanks I really need\n",
      "Detected Emotion: neutral\n",
      "Improv Partner Response: Anytime. It’s crazy how some people just don’t get it. So, what’s next for you? Are you going to do something for yourself, maybe treat yourself like the star you are?\n",
      "Press SPACE to start recording or stop cell execution to exit.\n",
      "Recording started.\n",
      "Recording stopped.\n",
      "Detected Dialog: yeah let's go buy some cake to\n",
      "Detected Emotion: neutral\n",
      "Improv Partner Response: Now we're talking! Cake's the perfect antidote for a day like this. Any particular flavor calling your name, or are we going all out and trying something wild?\n",
      "Press SPACE to start recording or stop cell execution to exit.\n",
      "Recording started.\n",
      "Recording stopped.\n",
      "Detected Dialog: how about vanilla\n",
      "Detected Emotion: neutral\n",
      "Improv Partner Response: Classic choice! You can't go wrong with vanilla; it’s like the comforting hug of flavors. Let’s grab that cake and maybe even some sprinkles to brighten things up! What else will we add to the celebration?\n",
      "Press SPACE to start recording or stop cell execution to exit.\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model and tokenizer for textual emotion analysis\n",
    "model_name = \"michellejieli/emotion_text_classifier\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "# Initialize the improv partner\n",
    "partner = ImprovPartner(\n",
    "    \"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\"\n",
    ")\n",
    "# Initialize the camera\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "if not cap.isOpened():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open camera\")\n",
    "while True:\n",
    "    try:\n",
    "        emotion, dialog = get_input_data(cap, model, tokenizer)\n",
    "        print(\"Detected Dialog:\", dialog)\n",
    "        print(\"Detected Emotion:\", emotion)\n",
    "        partner_response = partner.get_next_improv_response(\n",
    "            dialog, emotion\n",
    "        )\n",
    "        print(\"Improv Partner Response:\", partner_response)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Exiting...\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        break\n",
    "cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
