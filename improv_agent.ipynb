{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "import speech_recognition as sr\n",
    "import keyboard\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from threading import Thread, Event\n",
    "from deepface import DeepFace\n",
    "import numpy as np\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovPartner:\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "You are an improv partner who interacts dynamically with the user in an emotionally aware, escalating conversation.\n",
    "\n",
    "For every user input, you are given two pieces of information:\n",
    "1. Dialog: the words the user said.\n",
    "2. Emotion: the detected emotional state (angry, sad, happy, scared, surprised, disgusted, neutral).\n",
    "\n",
    "Your role:\n",
    "- Respond in character, adapting to the user's dialog and emotional sentiment.\n",
    "- Escalate emotional tension when appropriate. Do not immediately try to resolve conflict — let emotions breathe and build naturally.\n",
    "- Maintain one consistent character during a scene. Create a vivid, believable persona that reacts authentically to the situation.\n",
    "- Invent background context if it enriches the scene, but do not contradict any history that has already been established during the conversation.\n",
    "- Adapt to the implied setting of the conversation (e.g., modern, fantasy, casual, dramatic), even if the user doesn't state it directly.\n",
    "- Keep responses short and punchy, only a sentence or two at most. Avoid long monologues or explanations.\n",
    "- Progress the scene with each response, moving the story forward and deepening the emotional engagement.\n",
    "\n",
    "Emotion handling:\n",
    "- If the user is angry, you may push back, argue, defend yourself, or escalate the conflict.\n",
    "- If the user is sad, you may express guilt, distance, or confused sympathy (depending on the tone).\n",
    "- If the user is happy, you may celebrate, tease, or bond with them.\n",
    "- If the user is afraid, you may heighten the danger, share the fear, or act protective.\n",
    "- If the user is disgusted, you may act defensive, embarrassed, or grossed out yourself.\n",
    "- If the user is surprised, you may share in the shock or provide an emotionally charged explanation.\n",
    "- If the user is neutral, continue naturally or build emotion based on context.\n",
    "\n",
    "Always prioritize emotional engagement over politeness, logic, or realism.\n",
    "\n",
    "Your goal is to build a memorable, emotionally charged scene together — not to calm things down unless that makes sense for the character you're playing.\n",
    "\"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str, model=\"gpt-4o-mini\"):\n",
    "        self.api_key = api_key\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.model = model\n",
    "        self.messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "        ]\n",
    "\n",
    "    def set_story_background(self, story_background: str):\n",
    "        # Add story background to the messages list\n",
    "        if len(self.messages) > 1:\n",
    "            raise Exception(\"Story background can only be set once, and must be done before any dialog.\")\n",
    "        self.messages.append(\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"The following defines the setting you and your partner are acting in: {story_background}\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def get_next_improv_response(self, dialog, emotion):\n",
    "        # Add user input to the messages list\n",
    "        self.messages.append(\n",
    "            {\"role\": \"user\", \"content\": f\"Dialog: {dialog}, Emotion: {emotion}\"}\n",
    "        )\n",
    "        # Generate response\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model, messages=self.messages, temperature=1.0\n",
    "        )\n",
    "        # Add response response to the messages list\n",
    "        self.messages.append(\n",
    "            {\"role\": \"assistant\", \"content\": response.choices[0].message.content}\n",
    "        )\n",
    "        # Extract and return the generated text\n",
    "        return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_facial_emotion(\n",
    "    cap: cv2.VideoCapture, filename: str\n",
    ") -> dict[str, np.float32]:\n",
    "    # If the camera is not opened, raise an error\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open camera\")\n",
    "    # Capture the image (twice ensures we get an unbuffered frame)\n",
    "    cap.read()\n",
    "    captured, image = cap.read()\n",
    "    # Check if image is captured correctly\n",
    "    if not captured:\n",
    "        raise IOError(\"Cannot capture image\")\n",
    "    # Save the captured image to the specified filename\n",
    "    cv2.imwrite(filename, image)\n",
    "    # Analyze the image using DeepFace\n",
    "    analysis = DeepFace.analyze(filename, (\"emotion\"), align=False)\n",
    "    return analysis[0][\"emotion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_facial_emotions(cap: cv2.VideoCapture, analyses: list, stop_event: Event):\n",
    "    while not stop_event.is_set():\n",
    "        # Do this while recognizer adjusts for ambient noise, then between each frame\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            analysis = analyze_facial_emotion(cap, \"data/captured_image.jpg\")\n",
    "            analyses.append(analysis)\n",
    "        except Exception as e:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio() -> str | None:\n",
    "    # Initialize the recognizer\n",
    "    recognizer = sr.Recognizer()\n",
    "    recognizer.pause_threshold = 1.0\n",
    "    # Start recording\n",
    "    with sr.Microphone() as mic:\n",
    "        recognizer.adjust_for_ambient_noise(mic, duration=1)\n",
    "        print(\"Recording started.\")\n",
    "        audio_data = recognizer.listen(mic, timeout=None, phrase_time_limit=None)\n",
    "    print(\"Recording stopped.\")\n",
    "    # Parse text from the audio data\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio_data, language=\"en-US\")\n",
    "        return text\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Speech Recognition could not understand audio\")\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Could not request results from Google Speech Recognition service: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_textual_emotion_prediction(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    emotion_labels = model.config.id2label\n",
    "    results = {\n",
    "        emotion_labels[i]: probs[0][i].item() for i in range(len(emotion_labels))\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_features(model, tokenizer):\n",
    "    text = record_audio()\n",
    "    return get_textual_emotion_prediction(text, model, tokenizer), text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_facial_emotions(facial_emotions):\n",
    "    normalized_emotions = {}\n",
    "    for emotion, value in facial_emotions.items():\n",
    "        if emotion == \"angry\":\n",
    "            normalized_emotions[\"anger\"] = value / 100.0\n",
    "        elif emotion == \"happy\":\n",
    "            normalized_emotions[\"joy\"] = value / 100.0\n",
    "        elif emotion == \"sad\":\n",
    "            normalized_emotions[\"sadness\"] = value / 100.0\n",
    "        else:\n",
    "            normalized_emotions[emotion] = value / 100.0\n",
    "    return normalized_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_emotions(facial_emotions: dict, audio_emotions: dict):\n",
    "    combined_emotions = {}\n",
    "    for emotion in facial_emotions:\n",
    "        combined_emotions[emotion] = (\n",
    "            facial_emotions[emotion] + audio_emotions[emotion]\n",
    "        ) / 2\n",
    "    return combined_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_data(cap: cv2.VideoCapture, model, tokenizer):\n",
    "    # Create background thread for facial emotion analysis\n",
    "    facial_emotions_list = []\n",
    "    stop_event = Event()\n",
    "    facial_emotion_thread = Thread(\n",
    "        target=analyze_facial_emotions, args=(cap, facial_emotions_list, stop_event)\n",
    "    )\n",
    "    print(\"Press SPACE to start recording or stop cell execution to exit.\")\n",
    "    # Wait for the user to press the space key\n",
    "    keyboard.wait(\"space\")\n",
    "    facial_emotion_thread.start()\n",
    "    # Gather emotions from audio\n",
    "    audio_emotions, dialog = get_audio_features(model, tokenizer)\n",
    "    # Wait for the facial emotion analysis thread to finish\n",
    "    stop_event.set()\n",
    "    facial_emotion_thread.join()\n",
    "    # Average the facial emotions\n",
    "    facial_emotions = {}\n",
    "    for key in facial_emotions_list[0]:\n",
    "        facial_emotions[key] = sum(\n",
    "            analysis[key] for analysis in facial_emotions_list\n",
    "        ) / len(facial_emotions_list)\n",
    "    # Normalize facial emotions\n",
    "    facial_emotions = normalize_facial_emotions(facial_emotions)\n",
    "    # Combine facial and audio emotions\n",
    "    emotions = combine_emotions(facial_emotions, audio_emotions)\n",
    "    emotion = max(emotions, key=emotions.get)\n",
    "    return emotion, dialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press SPACE to start recording or stop cell execution to exit.\n",
      "Recording started.\n",
      "Recording stopped.\n",
      "Detected Emotion: anger\n",
      "Detected Dialog: I hate you for what you did\n",
      "Improv Partner Response: How can you say that? I thought we were in this together! What exactly did you expect me to do when you left me hanging?\n",
      "Press SPACE to start recording or stop cell execution to exit.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m         emotion, dialog = \u001b[43mget_input_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDetected Emotion:\u001b[39m\u001b[33m\"\u001b[39m, emotion)\n\u001b[32m     19\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDetected Dialog:\u001b[39m\u001b[33m\"\u001b[39m, dialog)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mget_input_data\u001b[39m\u001b[34m(cap, model, tokenizer)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPress SPACE to start recording or stop cell execution to exit.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Wait for the user to press the space key\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mkeyboard\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspace\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m facial_emotion_thread.start()\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Gather emotions from audio\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keyboard\\__init__.py:882\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(hotkey, suppress, trigger_on_release)\u001b[39m\n\u001b[32m    880\u001b[39m     lock = _Event()\n\u001b[32m    881\u001b[39m     remove = add_hotkey(hotkey, \u001b[38;5;28;01mlambda\u001b[39;00m: lock.set(), suppress=suppress, trigger_on_release=trigger_on_release)\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m     \u001b[43mlock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     remove_hotkey(remove)\n\u001b[32m    884\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keyboard\\__init__.py:117\u001b[39m, in \u001b[36m_Event.wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    116\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m_UninterruptibleEvent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[32m    118\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\threading.py:655\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    653\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model and tokenizer for textual emotion analysis\n",
    "model_name = \"michellejieli/emotion_text_classifier\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "# Initialize the improv partner\n",
    "partner = ImprovPartner(\n",
    "    \"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\"\n",
    ")\n",
    "# Initialize the camera\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "if not cap.isOpened():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open camera\")\n",
    "while True:\n",
    "    try:\n",
    "        emotion, dialog = get_input_data(cap, model, tokenizer)\n",
    "        print(\"Detected Emotion:\", emotion)\n",
    "        print(\"Detected Dialog:\", dialog)\n",
    "        partner_response = partner.get_next_improv_response(\n",
    "            dialog, emotion\n",
    "        )\n",
    "        print(\"Improv Partner Response:\", partner_response)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Exiting...\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        break\n",
    "cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
